# IRS-assisted Non-orthogonal SISO Communication Optimization

This project focuses on optimizing IRS-assisted non-orthogonal SISO communication using neural networks. 
The implementation includes supervised and self-supervised (goal-oriented learning/objective-driven unsupervised learning/
direct metric optimization) learning approaches to train neural networks for optimizing the communication system. 
The project is translated from MATLAB to Python and provides competitive results compared to traditional optimization 
algorithms.

## Project Structure

### Files
- **`SampleGeneration.py`**:  
  Generates datasets for training the neural network. It supports both supervised and self-supervised modes. 
The generated datasets are saved in `.pt` format for later use in training.

- **`Training.py`**:  
  Trains the neural network using the datasets generated by `SampleGeneration.py`. Supports both supervised and 
self-supervised training modes. The trained models are saved for evaluation.

- **`functionsNN.py`**:  
  Contains the implementation of the neural network classes and dataset classes. Includes methods for supervised and 
self-supervised learning.

- **`main.py`**:  
  Evaluates the trained neural network models and compares their performance with traditional optimization algorithms.

- **`functionsChannels.py`**:  
  Implements the channel generation functions used in the dataset creation process.

- **`README.md`**:  
  Provides an overview of the project, its structure, and usage instructions.

### Libraries Used
The project relies on the following Python libraries:
- **NumPy**: For numerical computations.
- **Matplotlib**: For plotting and visualizing results.
- **Torch (PyTorch)**: For building and training neural networks.
- **CVXPY**: For solving convex optimization problems.
- **SciPy**: For additional mathematical operations.
- **TQDM**: For progress bar visualization during dataset generation.
- **Random**: For generating random numbers.

## Results
The neural network models achieve competitive performance compared to traditional optimization algorithms:
- **SDR**: 14.99
- **NN supervised**: 14.50
- **NN self-supervised**: 14.52
- **NN supervised + self-supervised**: 14.60

## To-Do
- Extend the implementation to support beyond-diagonal IRS optimization.

## How to Use
1. **Generate Datasets**: Run `SampleGeneration.py` to create datasets for training.
2. **Train Models**: Use `Training.py` to train the neural network models.
3. **Evaluate Models**: Run `main.py` to evaluate the trained models and compare their performance with optimization algorithms.

## Versions:
    # 20250303: 
        # Added:
            # Code for Non-orthogonal SISO is translated from Matlab to Python.
            # The script SampleGeneration.py is used to generate samples for the training of 
                the neural network. The best performance are obtained when generating all the
                samples from a single setup.
            # The script functionsNN.py include:
                - Class for NN. The training method includes the options for supervised and 
                    self-supervised learning.
                - Classes for the datasets. Datasets for supervised and self-supervised learning
                    are created with the samples generated by SampleGeneration.py.
            # The script Training.py is used to train the neural network. The training is done
                with the samples generated by SampleGeneration.py.
            # The script main.py is used to evaluate the NN models trained and compare them
                with the results of the optimization algorithms.
        # Results:
            # The results from the NN are competitive if compared to the optimization algorithms 
                when evaluated in serveral random setups with random realizations:
                - SDR = 14.99
                - NN supervised = 14.50 (Model_M_32_K_5_setups_1_realiz_200000_supervised_Epochs_5_NN1_0.0001.pt)
                - NN self-supervised = 14.52 (Model_M_32_K_5_setups_1_realiz_200000_self_supervised_Epochs_5_NN1_0.0001.pt)
                - NN superv+self-superv = 14.60 (Model_M_32_K_5_setups_1_realiz_200000_self_supervised_Epochs_5_NN1_1e-05.pt)
            # The self-supervised dataset with 2x10^6 couldn't be used due to memory limitations.

    # To do:
        - Extend to Beyond Diagonal IRS optimization.


 
